{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa48965",
   "metadata": {},
   "source": [
    "# 🤖 MLflow Training Pipeline\n",
    "## FASE 3: Machine Learning + Experiment Tracking\n",
    "\n",
    "Este notebook implementa el pipeline de entrenamiento con MLflow tracking.\n",
    "\n",
    "### 🎯 **Objetivos:**\n",
    "1. **Configurar MLflow** tracking server\n",
    "2. **Feature Engineering** desde PostgreSQL\n",
    "3. **Entrenar modelos** (Random Forest, XGBoost, Linear Regression)\n",
    "4. **Comparar experimentos** y seleccionar el mejor\n",
    "5. **Registrar modelo** en MLflow Registry\n",
    "\n",
    "### 🏗️ **Arquitectura Hexagonal:**\n",
    "- **Domain**: Servicios de ML y reglas de negocio\n",
    "- **Ports**: Interfaces para ML y tracking\n",
    "- **Adapters**: MLflow, scikit-learn, PostgreSQL\n",
    "\n",
    "### 📋 **Prerequisitos:**\n",
    "- FASE 2 completada (PostgreSQL con datos)\n",
    "- 49,719 viajes en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879c4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setup FASE 3 completado!\n",
      "📅 Fecha: 2025-07-19 10:37:56\n",
      "🎯 Objetivo: Entrenar modelos con MLflow tracking\n"
     ]
    }
   ],
   "source": [
    "# 📦 Setup e imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import asyncpg\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar el directorio del proyecto al path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'taxi_duration_predictor'))\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🚀 Setup FASE 3 completado!\")\n",
    "print(f\"📅 Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"🎯 Objetivo: Entrenar modelos con MLflow tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae009804",
   "metadata": {},
   "source": [
    "## 🔧 **Paso 1: Configurar MLflow Tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95a4731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/19 10:39:35 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/07/19 10:39:35 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/07/19 10:39:35 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/07/19 10:39:36 INFO mlflow.tracking.fluent: Experiment with name 'taxi_duration_prediction' does not exist. Creating a new experiment.\n",
      "2025/07/19 10:39:36 INFO mlflow.tracking.fluent: Experiment with name 'taxi_duration_prediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 MLflow configurado:\n",
      "   Tracking URI: sqlite:///mlflow.db\n",
      "   Experiment: taxi_duration_prediction\n",
      "📡 PostgreSQL configurado para feature extraction\n",
      "✅ Configuración completa!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Configuración MLflow\n",
    "# Usar SQLite local para tracking (simple para el curso)\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"taxi_duration_prediction\")\n",
    "\n",
    "# Configuración de conexión PostgreSQL (reutilizar de FASE 2)\n",
    "AWS_ENDPOINT = \"taxi-duration-db.ckj7uy651uld.us-east-1.rds.amazonaws.com\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"taxiuser\"\n",
    "DB_PASSWORD = \"TaxiDB2025!\"\n",
    "\n",
    "print(\"🔬 MLflow configurado:\")\n",
    "print(f\"   Tracking URI: sqlite:///mlflow.db\")\n",
    "print(f\"   Experiment: taxi_duration_prediction\")\n",
    "print(\"📡 PostgreSQL configurado para feature extraction\")\n",
    "print(\"✅ Configuración completa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bcb321",
   "metadata": {},
   "source": [
    "## 📊 **Paso 2: Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79327645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Ejecuta: X, y, df = await extract_and_engineer_features(10000)\n"
     ]
    }
   ],
   "source": [
    "# 📊 Función para extraer datos y crear features\n",
    "async def extract_and_engineer_features(sample_size: int = 10000):\n",
    "    \"\"\"Extrae datos de PostgreSQL y crea features para ML\"\"\"\n",
    "    try:\n",
    "        print(f\"🔄 Extrayendo {sample_size} registros para entrenamiento...\")\n",
    "\n",
    "        # Conectar a PostgreSQL\n",
    "        conn = await asyncpg.connect(\n",
    "            host=AWS_ENDPOINT,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "\n",
    "        # Extraer muestra aleatoria\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            pickup_longitude, pickup_latitude,\n",
    "            dropoff_longitude, dropoff_latitude,\n",
    "            passenger_count, vendor_id,\n",
    "            pickup_datetime, trip_duration_seconds\n",
    "        FROM taxi_trips\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "\n",
    "        rows = await conn.fetch(query)\n",
    "        await conn.close()\n",
    "\n",
    "        # Convertir a DataFrame\n",
    "        df = pd.DataFrame([dict(row) for row in rows])\n",
    "        print(f\"📊 Datos extraídos: {df.shape}\")\n",
    "\n",
    "        # Feature Engineering\n",
    "        print(\"🔨 Creando features...\")\n",
    "\n",
    "        # 1. Distancia Haversine\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            R = 6371  # Radio de la Tierra en km\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "        df['distance_km'] = haversine_distance(\n",
    "            df['pickup_latitude'], df['pickup_longitude'],\n",
    "            df['dropoff_latitude'], df['dropoff_longitude']\n",
    "        )\n",
    "\n",
    "        # 2. Features temporales\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['hour_of_day'] = df['pickup_datetime'].dt.hour\n",
    "        df['day_of_week'] = df['pickup_datetime'].dt.dayofweek\n",
    "        df['month'] = df['pickup_datetime'].dt.month\n",
    "\n",
    "        # 3. Features categóricas\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['is_rush_hour'] = df['hour_of_day'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "\n",
    "        # 4. Target (convertir a minutos)\n",
    "        df['duration_minutes'] = df['trip_duration_seconds'] / 60\n",
    "\n",
    "        # Seleccionar features finales\n",
    "        feature_columns = [\n",
    "            'distance_km', 'passenger_count', 'vendor_id',\n",
    "            'hour_of_day', 'day_of_week', 'month',\n",
    "            'is_weekend', 'is_rush_hour'\n",
    "        ]\n",
    "\n",
    "        X = df[feature_columns]\n",
    "        y = df['duration_minutes']\n",
    "\n",
    "        print(f\"✅ Features creadas: {X.shape}\")\n",
    "        print(f\"📈 Features: {list(X.columns)}\")\n",
    "        print(f\"🎯 Target: duration_minutes (promedio: {y.mean():.1f} min)\")\n",
    "\n",
    "        return X, y, df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en feature engineering: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ✅ Ejecutar extracción\n",
    "print(\"💡 Ejecuta: X, y, df = await extract_and_engineer_features(10000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4baa5b",
   "metadata": {},
   "source": [
    "## 🤖 **Paso 3: Pipeline de Entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c43841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Función de entrenamiento lista\n"
     ]
    }
   ],
   "source": [
    "# 🤖 Función para entrenar y evaluar modelo\n",
    "def train_and_evaluate_model(X, y, model, model_name, hyperparams=None):\n",
    "    \"\"\"Entrena un modelo y registra en MLflow\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_experiment\"):\n",
    "        print(f\"🔄 Entrenando {model_name}...\")\n",
    "\n",
    "        # Dividir datos\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Normalizar features para algunos modelos\n",
    "        if model_name in ['LinearRegression']:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "\n",
    "        # Entrenar modelo\n",
    "        start_time = datetime.now()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "        # Predicciones\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Métricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        # Log en MLflow\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"features\", list(X.columns))\n",
    "\n",
    "        if hyperparams:\n",
    "            mlflow.log_params(hyperparams)\n",
    "\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "\n",
    "        # Guardar modelo\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"✅ {model_name} entrenado:\")\n",
    "        print(f\"   RMSE: {rmse:.2f} minutos\")\n",
    "        print(f\"   MAE: {mae:.2f} minutos\")\n",
    "        print(f\"   R²: {r2:.3f}\")\n",
    "        print(f\"   Tiempo: {training_time:.1f}s\")\n",
    "\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "\n",
    "print(\"💡 Función de entrenamiento lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ae9d4",
   "metadata": {},
   "source": [
    "## 🏁 **Paso 4: Ejecutar Experimentos**\n",
    "\n",
    "Ahora vamos a entrenar múltiples modelos y compararlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34314044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Ejecuta: results = await run_ml_experiments()\n"
     ]
    }
   ],
   "source": [
    "# 🏁 Ejecutar experimentos completos\n",
    "async def run_ml_experiments():\n",
    "    \"\"\"Ejecuta todos los experimentos de ML\"\"\"\n",
    "\n",
    "    print(\"🚀 Iniciando experimentos de ML...\")\n",
    "\n",
    "    # 1. Extraer datos y crear features\n",
    "    X, y, df = await extract_and_engineer_features(10000)\n",
    "\n",
    "    if X is None:\n",
    "        print(\"❌ Error en extracción de datos\")\n",
    "        return\n",
    "\n",
    "    # 2. Definir modelos a probar\n",
    "    models_to_test = [\n",
    "        {\n",
    "            'model': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'name': 'RandomForest',\n",
    "            'params': {'n_estimators': 100, 'random_state': 42}\n",
    "        },\n",
    "        {\n",
    "            'model': LinearRegression(),\n",
    "            'name': 'LinearRegression',\n",
    "            'params': {}\n",
    "        },\n",
    "        {\n",
    "            'model': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "            'name': 'XGBoost',\n",
    "            'params': {'n_estimators': 100, 'random_state': 42}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 3. Entrenar todos los modelos\n",
    "    results = []\n",
    "\n",
    "    for model_config in models_to_test:\n",
    "        result = train_and_evaluate_model(\n",
    "            X, y,\n",
    "            model_config['model'],\n",
    "            model_config['name'],\n",
    "            model_config['params']\n",
    "        )\n",
    "        results.append(result)\n",
    "        print(\"\" + \"-\"*50)\n",
    "\n",
    "    # 4. Resumen de resultados\n",
    "    print(\"\\n📊 **RESUMEN DE EXPERIMENTOS:**\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('rmse')\n",
    "\n",
    "    print(results_df[['model_name', 'rmse', 'mae', 'r2']].to_string(index=False))\n",
    "\n",
    "    best_model = results_df.iloc[0]\n",
    "    print(f\"\\n🏆 **MEJOR MODELO: {best_model['model_name']}**\")\n",
    "    print(f\"   RMSE: {best_model['rmse']:.2f} minutos\")\n",
    "    print(f\"   MAE: {best_model['mae']:.2f} minutos\")\n",
    "    print(f\"   R²: {best_model['r2']:.3f}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "print(\"💡 Ejecuta: results = await run_ml_experiments()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97708eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando experimentos de ML...\n",
      "🔄 Extrayendo 10000 registros para entrenamiento...\n",
      "📊 Datos extraídos: (10000, 8)\n",
      "🔨 Creando features...\n",
      "❌ Error en feature engineering: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable radians method\n",
      "❌ Error en extracción de datos\n",
      "📊 Datos extraídos: (10000, 8)\n",
      "🔨 Creando features...\n",
      "❌ Error en feature engineering: loop of ufunc does not support argument 0 of type decimal.Decimal which has no callable radians method\n",
      "❌ Error en extracción de datos\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUTAR TODOS LOS EXPERIMENTOS\n",
    "results = await run_ml_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6740e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Columnas en taxi_trips:\n",
      "   - id: character varying\n",
      "   - vendor_id: integer\n",
      "   - pickup_datetime: timestamp without time zone\n",
      "   - dropoff_datetime: timestamp without time zone\n",
      "   - passenger_count: integer\n",
      "   - pickup_longitude: numeric\n",
      "   - pickup_latitude: numeric\n",
      "   - dropoff_longitude: numeric\n",
      "   - dropoff_latitude: numeric\n",
      "   - store_and_fwd_flag: character varying\n",
      "   - trip_duration_seconds: numeric\n",
      "   - created_at: timestamp without time zone\n",
      "\n",
      "🔍 Muestra de datos:\n",
      "   id: id2875421\n",
      "   vendor_id: 2\n",
      "   pickup_datetime: 2016-03-14 17:24:55\n",
      "   dropoff_datetime: 2016-03-14 17:32:30\n",
      "   passenger_count: 1\n",
      "   pickup_longitude: -73.9821548\n",
      "   pickup_latitude: 40.7679367\n",
      "   dropoff_longitude: -73.9646301\n",
      "   dropoff_latitude: 40.7656021\n",
      "   store_and_fwd_flag: N\n",
      "   trip_duration_seconds: 455.00\n",
      "   created_at: 2025-07-19 14:14:20.619628\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Verificar estructura de la base de datos\n",
    "async def check_database_structure():\n",
    "    \"\"\"Verifica las columnas de la tabla\"\"\"\n",
    "    try:\n",
    "        conn = await asyncpg.connect(\n",
    "            host=AWS_ENDPOINT,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "\n",
    "        # Ver estructura de la tabla\n",
    "        columns = await conn.fetch(\"\"\"\n",
    "            SELECT column_name, data_type\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'taxi_trips'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"📋 Columnas en taxi_trips:\")\n",
    "        for col in columns:\n",
    "            print(f\"   - {col['column_name']}: {col['data_type']}\")\n",
    "\n",
    "        # Ver muestra de datos\n",
    "        sample = await conn.fetchrow(\"SELECT * FROM taxi_trips LIMIT 1\")\n",
    "        print(f\"\\n🔍 Muestra de datos:\")\n",
    "        for key, value in sample.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "\n",
    "        await conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "await check_database_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b56cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Función corregida lista!\n"
     ]
    }
   ],
   "source": [
    "# 📊 Función CORREGIDA para extraer datos y crear features\n",
    "async def extract_and_engineer_features_fixed(sample_size: int = 10000):\n",
    "    \"\"\"Extrae datos de PostgreSQL y crea features para ML\"\"\"\n",
    "    try:\n",
    "        print(f\"🔄 Extrayendo {sample_size} registros para entrenamiento...\")\n",
    "\n",
    "        # Conectar a PostgreSQL\n",
    "        conn = await asyncpg.connect(\n",
    "            host=AWS_ENDPOINT,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "\n",
    "        # Extraer muestra aleatoria\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            pickup_longitude, pickup_latitude,\n",
    "            dropoff_longitude, dropoff_latitude,\n",
    "            passenger_count, vendor_id,\n",
    "            pickup_datetime, trip_duration_seconds\n",
    "        FROM taxi_trips\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "\n",
    "        rows = await conn.fetch(query)\n",
    "        await conn.close()\n",
    "\n",
    "        # Convertir a DataFrame y manejar tipos Decimal\n",
    "        df = pd.DataFrame([dict(row) for row in rows])\n",
    "\n",
    "        # Convertir Decimal a float\n",
    "        numeric_columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'trip_duration_seconds']\n",
    "        for col in numeric_columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "        print(f\"📊 Datos extraídos: {df.shape}\")\n",
    "\n",
    "        # Feature Engineering\n",
    "        print(\"🔨 Creando features...\")\n",
    "\n",
    "        # 1. Distancia Haversine\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            R = 6371  # Radio de la Tierra en km\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "        df['distance_km'] = haversine_distance(\n",
    "            df['pickup_latitude'], df['pickup_longitude'],\n",
    "            df['dropoff_latitude'], df['dropoff_longitude']\n",
    "        )\n",
    "\n",
    "        # 2. Features temporales\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['hour_of_day'] = df['pickup_datetime'].dt.hour\n",
    "        df['day_of_week'] = df['pickup_datetime'].dt.dayofweek\n",
    "        df['month'] = df['pickup_datetime'].dt.month\n",
    "\n",
    "        # 3. Features categóricas\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        df['is_rush_hour'] = df['hour_of_day'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "\n",
    "        # 4. Target (convertir a minutos)\n",
    "        df['duration_minutes'] = df['trip_duration_seconds'] / 60\n",
    "\n",
    "        # Seleccionar features finales\n",
    "        feature_columns = [\n",
    "            'distance_km', 'passenger_count', 'vendor_id',\n",
    "            'hour_of_day', 'day_of_week', 'month',\n",
    "            'is_weekend', 'is_rush_hour'\n",
    "        ]\n",
    "\n",
    "        X = df[feature_columns]\n",
    "        y = df['duration_minutes']\n",
    "\n",
    "        print(f\"✅ Features creadas: {X.shape}\")\n",
    "        print(f\"📈 Features: {list(X.columns)}\")\n",
    "        print(f\"🎯 Target: duration_minutes (promedio: {y.mean():.1f} min)\")\n",
    "\n",
    "        return X, y, df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en feature engineering: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "print(\"✅ Función corregida lista!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d012cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando experimentos de ML...\n",
      "🔄 Extrayendo 10000 registros para entrenamiento...\n",
      "📊 Datos extraídos: (10000, 8)\n",
      "🔨 Creando features...\n",
      "✅ Features creadas: (10000, 8)\n",
      "📈 Features: ['distance_km', 'passenger_count', 'vendor_id', 'hour_of_day', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour']\n",
      "🎯 Target: duration_minutes (promedio: 14.0 min)\n",
      "🔄 Entrenando RandomForest...\n",
      "📊 Datos extraídos: (10000, 8)\n",
      "🔨 Creando features...\n",
      "✅ Features creadas: (10000, 8)\n",
      "📈 Features: ['distance_km', 'passenger_count', 'vendor_id', 'hour_of_day', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour']\n",
      "🎯 Target: duration_minutes (promedio: 14.0 min)\n",
      "🔄 Entrenando RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/19 10:43:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/19 10:43:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/19 10:43:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest entrenado:\n",
      "   RMSE: 6.62 minutos\n",
      "   MAE: 4.27 minutos\n",
      "   R²: 0.681\n",
      "   Tiempo: 5.1s\n",
      "--------------------------------------------------\n",
      "🔄 Entrenando LinearRegression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/19 10:43:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/19 10:43:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/19 10:43:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LinearRegression entrenado:\n",
      "   RMSE: 7.47 minutos\n",
      "   MAE: 4.85 minutos\n",
      "   R²: 0.595\n",
      "   Tiempo: 0.1s\n",
      "--------------------------------------------------\n",
      "🔄 Entrenando XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/19 10:43:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/19 10:44:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/19 10:44:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost entrenado:\n",
      "   RMSE: 6.85 minutos\n",
      "   MAE: 4.31 minutos\n",
      "   R²: 0.659\n",
      "   Tiempo: 0.3s\n",
      "--------------------------------------------------\n",
      "\n",
      "📊 **RESUMEN DE EXPERIMENTOS:**\n",
      "      model_name     rmse      mae       r2\n",
      "    RandomForest 6.623458 4.274288 0.681264\n",
      "         XGBoost 6.849632 4.305443 0.659124\n",
      "LinearRegression 7.468528 4.847471 0.594741\n",
      "\n",
      "🏆 **MEJOR MODELO: RandomForest**\n",
      "   RMSE: 6.62 minutos\n",
      "   MAE: 4.27 minutos\n",
      "   R²: 0.681\n",
      "\n",
      "🔬 **MLflow UI:** mlflow ui --backend-store-uri sqlite:///mlflow.db\n"
     ]
    }
   ],
   "source": [
    "# 🚀 EJECUTAR EXPERIMENTOS CORREGIDOS\n",
    "async def run_ml_experiments_fixed():\n",
    "    \"\"\"Ejecuta todos los experimentos de ML con función corregida\"\"\"\n",
    "\n",
    "    print(\"🚀 Iniciando experimentos de ML...\")\n",
    "\n",
    "    # 1. Extraer datos y crear features (versión corregida)\n",
    "    X, y, df = await extract_and_engineer_features_fixed(10000)\n",
    "\n",
    "    if X is None:\n",
    "        print(\"❌ Error en extracción de datos\")\n",
    "        return\n",
    "\n",
    "    # 2. Definir modelos a probar\n",
    "    models_to_test = [\n",
    "        {\n",
    "            'model': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'name': 'RandomForest',\n",
    "            'params': {'n_estimators': 100, 'random_state': 42}\n",
    "        },\n",
    "        {\n",
    "            'model': LinearRegression(),\n",
    "            'name': 'LinearRegression',\n",
    "            'params': {}\n",
    "        },\n",
    "        {\n",
    "            'model': xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0),\n",
    "            'name': 'XGBoost',\n",
    "            'params': {'n_estimators': 100, 'random_state': 42}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 3. Entrenar todos los modelos\n",
    "    results = []\n",
    "\n",
    "    for model_config in models_to_test:\n",
    "        result = train_and_evaluate_model(\n",
    "            X, y,\n",
    "            model_config['model'],\n",
    "            model_config['name'],\n",
    "            model_config['params']\n",
    "        )\n",
    "        results.append(result)\n",
    "        print(\"\" + \"-\"*50)\n",
    "\n",
    "    # 4. Resumen de resultados\n",
    "    print(\"\\n📊 **RESUMEN DE EXPERIMENTOS:**\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('rmse')\n",
    "\n",
    "    print(results_df[['model_name', 'rmse', 'mae', 'r2']].to_string(index=False))\n",
    "\n",
    "    best_model = results_df.iloc[0]\n",
    "    print(f\"\\n🏆 **MEJOR MODELO: {best_model['model_name']}**\")\n",
    "    print(f\"   RMSE: {best_model['rmse']:.2f} minutos\")\n",
    "    print(f\"   MAE: {best_model['mae']:.2f} minutos\")\n",
    "    print(f\"   R²: {best_model['r2']:.3f}\")\n",
    "\n",
    "    print(f\"\\n🔬 **MLflow UI:** mlflow ui --backend-store-uri sqlite:///mlflow.db\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# ¡EJECUTAR EXPERIMENTOS!\n",
    "results = await run_ml_experiments_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec70f5",
   "metadata": {},
   "source": [
    "## 📋 **Resumen de FASE 3**\n",
    "\n",
    "### ✅ **Pasos a ejecutar:**\n",
    "1. **Ejecutar setup** (celda 2)\n",
    "2. **Configurar MLflow** (celda 4) \n",
    "3. **Ejecutar experimentos**: `results = await run_ml_experiments()`\n",
    "\n",
    "### 🎯 **Qué lograremos:**\n",
    "- **MLflow tracking** de experimentos\n",
    "- **Comparación** de 3 modelos diferentes\n",
    "- **Feature engineering** profesional\n",
    "- **Métricas standardizadas** (RMSE, MAE, R²)\n",
    "\n",
    "### 🔧 **Para ver MLflow UI:**\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```\n",
    "\n",
    "### 🎓 **Para el curso:**\n",
    "*\"Implementamos el adaptador MLModelService con scikit-learn y XGBoost, demostrando cómo la arquitectura hexagonal nos permite comparar diferentes algoritmos sin cambiar la lógica de negocio. MLflow nos da tracking profesional de experimentos.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
